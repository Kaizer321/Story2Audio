{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Clone the nari-labs/Dia library locally","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/nari-labs/dia.git\n!pip install -q ./dia\nprint(\"Dia library and dependencies installed.\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Install required libraries","metadata":{}},{"cell_type":"code","source":"!pip install soundfile ffmpeg-python grpcio grpcio-tools gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import gradio as gr\nimport soundfile as sf\nfrom dia.model import Dia\nimport torch\nimport numpy as np\nimport subprocess\nimport os\nimport random\nimport re\nimport tempfile\nimport ffmpeg\nfrom io import BytesIO","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:39:35.446835Z","iopub.execute_input":"2025-05-05T17:39:35.447497Z","iopub.status.idle":"2025-05-05T17:39:35.451513Z","shell.execute_reply.started":"2025-05-05T17:39:35.447471Z","shell.execute_reply":"2025-05-05T17:39:35.450564Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Gradio App","metadata":{}},{"cell_type":"code","source":"DEFAULT_WPM = 150\n\ndef check_device():\n    return \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ndef load_model(model_name):\n    if not hasattr(load_model, \"model\"):\n        load_model.model = Dia.from_pretrained(model_name)\n    return load_model.model\n\ndef generate_audio(model, text, seed=42):\n    torch.manual_seed(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n    audio = model.generate(text)\n    return audio\n\ndef save_audio(audio, file_name, sr=44100):\n    sf.write(file_name, audio, sr)\n\ndef adjust_audio_for_unified_speaker(input_audio, speed_factor=1.0):\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as input_temp_file:\n        input_temp_file.write(input_audio.read())\n        input_temp_file_path = input_temp_file.name\n\n    with tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\") as output_temp_file:\n        output_temp_file_path = output_temp_file.name\n\n    try:\n        ffmpeg.input(input_temp_file_path).output(\n            output_temp_file_path,\n            filter_complex=(\n                f\"atempo={speed_factor},\"\n                f\"equalizer=f=1000:t=q:w=1:g=-5,\"\n                f\"equalizer=f=2000:t=q:w=1:g=-5,\"\n                f\"equalizer=f=5000:t=q:w=1:g=-5,\"\n                f\"dynaudnorm,\"\n                f\"loudnorm=I=-16:TP=-1.5:LRA=11\"\n            )\n        ).run(overwrite_output=True)\n\n        with open(output_temp_file_path, 'rb') as f:\n            adjusted_audio = BytesIO(f.read())\n        adjusted_audio.seek(0)\n        return adjusted_audio\n\n    except ffmpeg.Error as e:\n        raise gr.Error(f\"An error occurred while processing the audio: {e}\")\n\ndef split_sentences(text):\n    sentences = re.split(r'(?<=[.!?])\\s+', text.strip())\n    return [s.strip() for s in sentences if s.strip()]\n\ndef batch_sentences(sentences, batch_size):\n    return [sentences[i:i+batch_size] for i in range(0, len(sentences), batch_size)]\n\ndef generate_silence(duration, sr=44100):\n    return np.zeros(int(duration * sr), dtype=np.float32)\n\ndef calculate_wpm_from_audio_duration(text, duration_seconds):\n    word_count = len(text.split())\n    return (word_count / duration_seconds) * 60 if duration_seconds > 0 else DEFAULT_WPM\n\ndef generate_narration(text_input, progress=gr.Progress()):\n    if os.path.exists(\"audio_parts\"):\n        for file in os.listdir(\"audio_parts\"):\n            os.remove(os.path.join(\"audio_parts\", file))\n        os.rmdir(\"audio_parts\")\n\n    os.makedirs(\"audio_parts\", exist_ok=True)\n\n    device = check_device()\n    model = load_model(\"nari-labs/Dia-1.6B\")\n    seed_value = 42\n\n    sentences = split_sentences(text_input)\n    sentence_batches = batch_sentences(sentences, 4)\n    total_batches = len(sentence_batches)\n\n    final_audio = np.array([], dtype=np.float32)\n\n    for idx, batch in enumerate(sentence_batches):\n        progress((idx+1, total_batches), desc=\"Generating narration\")\n\n        tagged_sentences = \" \".join(f\"[S1] {s}\" for s in batch)\n        audio = generate_audio(model, tagged_sentences, seed=seed_value)\n\n        part_filename = f\"audio_parts/batch_{idx}.wav\"\n        save_audio(audio, part_filename)\n\n        y, sr = sf.read(part_filename)\n        final_audio = np.concatenate((final_audio, y))\n\n        pause = generate_silence(1, sr)\n        final_audio = np.concatenate((final_audio, pause))\n\n    combined_filename = \"output_combined.wav\"\n    save_audio(final_audio, combined_filename)\n\n    for file in os.listdir(\"audio_parts\"):\n        os.remove(os.path.join(\"audio_parts\", file))\n    os.rmdir(\"audio_parts\")\n\n    audio_info = sf.info(combined_filename)\n    duration_seconds = audio_info.frames / audio_info.samplerate\n    current_wpm = calculate_wpm_from_audio_duration(text_input, duration_seconds)\n\n    return (\n        combined_filename,\n        text_input,\n        gr.update(value=current_wpm, visible=True),\n        gr.update(visible=True),\n        f\"Current WPM: {current_wpm:.2f}\"\n    )\n\ndef adjust_speed(text_input, new_wpm):\n    audio_file_path = \"output_combined.wav\"\n    if not os.path.exists(audio_file_path):\n        raise gr.Error(\"No existing narration found\")\n\n    audio_info = sf.info(audio_file_path)\n    duration_seconds = audio_info.frames / audio_info.samplerate\n    current_wpm = calculate_wpm_from_audio_duration(text_input, duration_seconds)\n    speed_factor = new_wpm / current_wpm\n\n    adjusted_audio_path = adjust_audio_for_unified_speaker(audio_file_path, speed_factor=speed_factor)\n    \n    adjusted_audio_info = sf.info(adjusted_audio_path)\n    adjusted_duration = adjusted_audio_info.frames / adjusted_audio_info.samplerate\n    adjusted_wpm = calculate_wpm_from_audio_duration(text_input, adjusted_duration)\n\n    return (\n        adjusted_audio_path,\n        f\"Current WPM: {adjusted_wpm:.2f}\",\n        f\"Speed Factor: {speed_factor:.2f}\"\n    )\n\nwith gr.Blocks(title=\"üéôÔ∏è EchoTales: The Voice of Stories\") as app:\n    gr.Markdown(\"# üéôÔ∏è EchoTales: The Voice of Stories\")\n\n    with gr.Row():\n        with gr.Column():\n            text_input = gr.TextArea(label=\"Enter your story text here:\", lines=10)\n            generate_btn = gr.Button(\"Generate Narration\")\n\n            audio_output = gr.Audio(label=\"Generated Narration\", interactive=False)\n\n            generated_text_state = gr.State()\n\n            with gr.Group(visible=False) as speed_adjust_group:\n                wpm_slider = gr.Slider(minimum=50, maximum=300, value=DEFAULT_WPM, label=\"Adjust WPM\")\n                wpm_display = gr.Textbox(label=\"Current WPM\")\n                speed_factor_display = gr.Textbox(label=\"Speed Adjustment Factor\")\n                adjust_btn = gr.Button(\"Adjust Speed\")\n\n            device_info = gr.Textbox(label=\"Device\", value=f\"Running on: {check_device()}\", interactive=False)\n\n    generate_btn.click(\n        fn=generate_narration,\n        inputs=text_input,\n        outputs=[audio_output, generated_text_state, wpm_slider, adjust_btn, wpm_display],\n    ).then(\n        lambda: gr.update(visible=False),\n        inputs=None,\n        outputs=speed_adjust_group\n    )\n\n    adjust_btn.click(\n        fn=adjust_speed,\n        inputs=[generated_text_state, wpm_slider],\n        outputs=[audio_output, wpm_display, speed_factor_display]\n    )\n\nif __name__ == \"__main__\":\n    app.launch(share=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-05T17:48:51.770530Z","iopub.execute_input":"2025-05-05T17:48:51.771063Z","iopub.status.idle":"2025-05-05T17:48:54.025337Z","shell.execute_reply.started":"2025-05-05T17:48:51.771034Z","shell.execute_reply":"2025-05-05T17:48:54.024581Z"}},"outputs":[{"name":"stdout","text":"* Running on local URL:  http://127.0.0.1:7864\n* Running on public URL: https://215ba56c74c0a265ec.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<div><iframe src=\"https://215ba56c74c0a265ec.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}